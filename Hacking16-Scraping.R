# Web Scraping and Building a Corpus, Twitter and Twitterature
# March 24, 2020

scanner<-function(filename){
  library(stringr)
  lines<-scan(filename,what="char",sep="\n")
  words<-tolower(unlist(str_split(lines,boundary("word"))))
  bagofwords<-words[which(words!="")]
  return(bagofwords)
}

getwd()
list.files()
scanner("TRY A TEXT FILE HERE")

############################################

# Scraping, the hard way...
url2<-"http://www.gutenberg.org/files/1934/1934-h/1934-h.htm"
tmp2 <- readLines(url2, warn = F)
grep("\\<h3\\>",x = tmp2,value=T) # poem titles
title.index <- grep("\\<h3\\>",x = tmp2) # indexing poem titles
tmp2[title.index[3]]
poem1<-tmp2[title.index[3]:(title.index[4]-1)]
gsub("<[a-z]")

# Now I have to grep out the tags... Ugh...

############################################

# It's much better to use a package that can parse XML/HTML
library(rvest)
library(magrittr) # not necessary but all the scraping tutorials like to use it with rvest...

# Example from rvest documentation
movie <- read_html("https://www.imdb.com/title/tt0069398/fullcredits?ref_=tt_cl_sm#cast") # Godard's "Tout Va Bien"
cast <- html_nodes(movie, "#fullcredits_content td:nth-child(2) a")
html_text(cast)
html_name(cast)
html_attrs(cast)
html_attr(cast, "class")

nyt<-read_html("https://www.nytimes.com/section/todayspaper?redirect_uri=https%3A%2F%2Fwww.nytimes.com%2F")
headlines<-html_nodes(nyt,".e1f68otr0")
gsub("\n","",html_text(headlines))

# Trying the same with Lou's List (inspired by a Michele CLaibourn tutorial for StatLab)
lous.english<-read_html("https://louslist.org/page.php?Semester=1208&Type=Group&Group=English")
nodes<-html_nodes(lous.english,".CourseNum span")
courses<-html_text(nodes)
courses<-gsub("^\\s+","",courses)

############################################

# Writing out all those lines can be avoided with a pipe operator.

library(magrittr)

"Ceci n'est pas un pipe" %>% nchar()
"Ceci n'est pas un pipe" %>% gsub("un", "une", .) # using the dot as a placeholder

"Ceci n'est pas un pipe" %>% gsub("un", "une", .) %>% nchar

############################################

# Practice with magrittr... Rewrite our scanner function. 

scanner<-function(filename){
  library(stringr); library(magrittr)
  bagofwords<-scan(filename,what="char",sep="\n") %>%
    str_split(boundary("word")) %>% unlist() %>% tolower() %>%
    .[which(.!="")] # Again, when the LHS is needed at a position other than the first, use the dot ('.') as placeholder. 
  return(bagofwords)
}

############################################

# Real-Life Scraping
# Brad Collecting Texts for Class
# Example with Isaac Watts

# Watts
wattspage<-read_html("http://quod.lib.umich.edu/e/ecco/004819385.0001.000?rgn=main;view=fulltext")
lines<-html_nodes(wattspage,"h4 , h3 , .line") %>% 
  # %>% is the pipe operator, it feeds the result to the next function: html_text()
  html_text() # That gets titles and lines. Assigning them, at start of this 
  #chain, to lines

# I suppose all this could be added to the pipeline... see below
lines<-gsub("\\n"," ",lines) # clean up line breaks, see page image: http://quod.lib.umich.edu/e/ecco/004819385.0001.000/64
lines<-c(lines,"END")

# All in pipeline. Hadley Wickham would be so proud...
# wattspage<-read_html("http://quod.lib.umich.edu/e/ecco/004819385.0001.000?rgn=main;view=fulltext")
# lines<-html_nodes(wattspage,"h4 , h3 , .line") %>% 
#   # %>% is the pipe operator, it feeds the result to the next function: html_text()
#   html_text() %>% gsub("\\n"," ",.) %>% c("END")

# Which of these lines are titles? (Looking in the object lines)
# Zap some subtitles
lines<-lines[-709] # not a title
lines<-lines[-747] # not a title
titles<-c(1,23,57,91,117,143,161,191,225,251,277,295,321,347,373,399,421,447,
          473,499,517,539,573,591,609,627,645,663,681,692,709,734,747,768,787,804,829,857,898,915,972)


# that's the wimpy way (I indexed titles by hand). Get the titles?
titles.scraped.index<-html_nodes(wattspage,"h3")
titles.scraped<-html_text(titles.scraped.index)
titles.scraped 

# initialize lists for loop
poems.l<-list()
poemspasted.l<-list()

# loop through scraped data
for(i in 1:length(titles)){
  if(i < length(titles)){
    title<-lines[titles[i]]
    start<-titles[i]+1
    end<-titles[i+1]-1
    poem<-lines[start:end]
    poempasted<-paste(poem,collapse="\n")
    poems.l[[title]]<-poem
    poemspasted.l[[title]]<-poempasted
  }
}

filenames<-gsub("\\s|\\.|,|;", "",tolower(names(poemspasted.l)))

# Write into files and leave on desktop in a folder
dir.create("~/Desktop/watts")
setwd("~/Desktop/watts")

for(i in 1:length(filenames)){
  wfile<-file(paste0(filenames[i],".txt"))
  writeLines(poemspasted.l[[i]],wfile)
  close(wfile)
}

############################################

# More functions? 

# This function extracts all the files from a directory (dir), turns them into bags of words, 
# and puts each file's words into a list
getcorpus<-function(dir,type=".txt"){
  curr.folder<-getwd()
  setwd(dir)
  corpus<-list()
  files<-list.files(pattern=type)
  for(i in 1:length(files)){
    text<-scan(files[i],what="char",sep="\n")
    text<-paste(text,collapse=" ")
    lowertext<-tolower(text)
    text.words<-unlist(strsplit(lowertext,"\\W"))
    text.words<-text.words[which(text.words!="")]
    corpus[[files[i]]]<-text.words
  }
  setwd(curr.folder)
  return(corpus)
}

############################################
############################################

# Cleaning up Blake: note this is not scraping, rather text processing. 
# I already have the Blake file... But this is an example of cleaning up texts, writing them out as files

setwd("~/projects/texts/blake")
songs<-scan("blake-songs-all.txt",what="char",sep="\n")
songs<-gsub("\\t","",songs)
songs<-c(songs,"END")
titles<-c(6,28,39,72,95,126,140,166,176,186,200,236,259,273,325,356,374,388,410,451,473,
          501,515,534,589,645,660,670,680,703,721,748,757,766,773,787,805,824,850,860,878,
          905,942,961,993,1007,1016)

songs.l<-list()
songspasted.l<-list()

for(i in 1:length(titles)){
  if(i < length(titles)){
    title<-songs[titles[i]]
    start<-titles[i]+1
    end<-titles[i+1]-1
    song<-songs[start:end]
    songpasted<-paste(song,collapse="\n")
    songs.l[[title]]<-song
    songspasted.l[[title]]<-songpasted
  }
}

filenames<-names(songspasted.l)

# Write into files
for(i in 1:length(filenames)){
  wfile<-file(paste0(filenames[i],".txt"), "w")
  writeLines(songspasted.l[[i]],wfile)
  close(wfile)
}

#################################

# Last example: Getting Jane Austen fan fiction

# Build a "bits of ivory corpus": Emma rewrites

urls<-c("http://www.pemberley.com/derby/oldc/kathyf1.htm","http://www.pemberley.com/derby/olde/rose1.htm","http://www.pemberley.com/derby/jimmy8.iada.html","http://www.pemberley.com/derby/oldb/kathy1.htm","http://www.pemberley.com/derby/oldc/laraine1.htm","http://www.pemberley.com/derby/oldc/martine1.htm","http://www.pemberley.com/derby/olda/susanc1.htm","http://www.pemberley.com/derby/olda/cass3.htm","http://www.pemberley.com/derby/aeliz2.htm","http://www.pemberley.com/derby/oldb/kat3.htm","http://www.pemberley.com/derby/olde/dee1.htm","http://www.pemberley.com/derby/olda/kali1.htm","http://www.pemberley.com/derby/kfay1.kd.html","http://www.pemberley.com/derby/deedee4.ew.html","http://www.pemberley.com/derby/ulrike.mds1.html","http://www.pemberley.com/derby/ulrike.lmu1.html","http://www.pemberley.com/derby/helena6.mr.html","http://www.pemberley.com/derby/sara-clare.cah.html","http://www.pemberley.com/derby/radhika.hsmsh.html","http://www.pemberley.com/derby/helena6.ahit.html")
bitsofivory.l<-list()
for(i in 1:length(urls)){
  bit<-read_html(urls[i])
  elements<-html_nodes(bit,"p")
  text<-html_text(elements)
  title<-text[1]
  text<-gsub("Â© \\d{4} Copyright held by the author","",text) #remove copyright
  text<-paste(text,collapse=" ")
  lowertext<-tolower(text)
  text.words<-unlist(strsplit(lowertext,"\\W"))
  text.words<-text.words[which(text.words!="")]
  bitsofivory.l[[title]]<-text.words
}
  
  ############################################
  
  # It's no longer possible to do scraping exercise which used to be below, because the Blake Archive has 
  # changed ... Although I suppose I could use the WayBack machine:
  # Yup: https://web.archive.org/web/20150302225047/http://www.blakearchive.org/exist/blake/archive/erdman.xq?id=b1.5
  
  ## No longer a good URL below. Replace with link above and uncomment? -- The Archive has removed 
  ## its flat web-page transcription of the Erdman edition. It's harder now to scrape!
  
  # url <- "http://www.blakearchive.org/exist/blake/archive/erdman.xq?id=b1.5#top"
  # tmp <- readLines(url, warn = F)
  # title.index <- grep("\\<h5\\>",x = tmp) # indexing poem titles
  # poem1<-tmp[title.index[1]:(title.index[2]-1)] # first poem (but with markup)
  
}

############################################